# Medium - Math & Neural Network

This code is part of my post on **[medium](https://medium.com/@omaraflak/math-neural-network-from-scratch-in-python-d6da9f29ce65)**.

# Run it

```shell
python example_xor.py
python example_conv.py
```



----

> Make your own machine learning library.



![img](./Pic/1.jpg)

Photo by [Mathew Schwartz](https://unsplash.com/@cadop?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

In this post we will go through the math of machine learning and code from scratch, in Python, a small library to build neural networks with a variety of layers (Fully Connected, Convolutional, etc.). Eventually, we will be able to write something like :

> åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨Pythonä¸­ä»å¤´å¼€å§‹äº†è§£ç”¨äºæ„å»ºå…·æœ‰å„ç§å±‚ç¥ç»ç½‘ç»œï¼ˆå®Œå…¨è¿æ¥ï¼Œå·ç§¯ç­‰ï¼‰çš„å°å‹åº“ä¸­çš„æœºå™¨å­¦ä¹ å’Œä»£ç ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å°†èƒ½å¤Ÿå†™å‡ºå¦‚ä¸‹å†…å®¹ï¼š
>

```
net = Network();
net.add(FCLayer((1,2), (1,3)));
net.add(ActivationLayer((1,3), tanth, tanh_prime));
net.add(FCLay((1,3), (1,1)));
net.add(ActivationLayer((1,1), tanth, tanh_prime));
```

3-layer neural network



Iâ€™m assuming you already have *some* knowledge about neural networks. The purpose here is not to explain why we make these models, but to show **how to make a proper implementation**.

> å‡è®¾ä½ å¯¹ç¥ç»ç½‘ç»œå·²ç»æœ‰ä¸€å®šçš„äº†è§£ï¼Œè¿™ç¯‡æ–‡ç« çš„ç›®çš„ä¸æ˜¯è§£é‡Šä¸ºä»€ä¹ˆæ„å»ºè¿™äº›æ¨¡å‹ï¼Œè€Œæ˜¯è¦è¯´æ˜**å¦‚ä½•æ­£ç¡®å®ç°**ã€‚

### Layer by Layer  **é€å±‚**

We need to keep in mind the big picture here :

1. We feed **input** data into the neural network.
2. The data flows **from layer to layer** until we have the **output**.
3. Once we have the output, we can calculate the **error** which is a **scalar**.
4. Finally we can adjust a given parameter (weight or bias) by subtracting the **derivative** of the error with respect to the parameter itself.
5. We iterate through that process.

>æˆ‘ä»¬è¿™é‡Œéœ€è¦ç‰¢è®°æ•´ä¸ªæ¡†æ¶ï¼š
>\1.    å°†æ•°æ®**è¾“å…¥**ç¥ç»ç½‘ç»œ
>\2.    åœ¨å¾—å‡ºè¾“å‡ºä¹‹å‰ï¼Œæ•°æ®**ä»ä¸€å±‚æµå‘ä¸‹ä¸€å±‚**ã€‚
>\3.    ä¸€æ—¦å¾—åˆ°è¾“å‡ºï¼Œå°±å¯ä»¥è®¡ç®—å‡ºä¸€ä¸ª**æ ‡é‡è¯¯å·®**ã€‚
>\4.    æœ€åï¼Œå¯ä»¥é€šè¿‡ç›¸å¯¹äºå‚æ•°æœ¬èº«å‡å»è¯¯å·®çš„**å¯¼æ•°**æ¥è°ƒæ•´ç»™å®šå‚æ•°ï¼ˆæƒé‡æˆ–åå·®ï¼‰ã€‚
>\5.    éå†æ•´ä¸ªè¿‡ç¨‹ã€‚

The most important step is the **4th**. We want to be able to have as many layers as we want, and of any type. But if we modify/add/remove one layer from the network, the output of the network is going to change, which is going to change the error, which is going to change the derivative of the error with respect to the parameters. We need to be able to compute the derivatives regardless of the network architecture, regardless of the activation functions, regardless of the loss we use.

In order to achieve that, we must implement **each layer separately**.

>æœ€é‡è¦çš„ä¸€æ­¥æ˜¯**ç¬¬å››æ­¥**ã€‚ æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿæ‹¥æœ‰ä»»æ„æ•°é‡çš„å±‚ï¼Œä»¥åŠä»»ä½•ç±»å‹çš„å±‚ã€‚ ä½†æ˜¯å¦‚æœä¿®æ”¹/æ·»åŠ /åˆ é™¤ç½‘ç»œä¸­çš„ä¸€ä¸ªå±‚ï¼Œç½‘ç»œçš„è¾“å‡ºå°†ä¼šæ”¹å˜ï¼Œè¯¯å·®ä¹Ÿå°†æ”¹å˜ï¼Œè¯¯å·®ç›¸å¯¹äºå‚æ•°çš„å¯¼æ•°ä¹Ÿå°†æ”¹å˜ã€‚æ— è®ºç½‘ç»œæ¶æ„å¦‚ä½•ã€æ¿€æ´»å‡½æ•°å¦‚ä½•ã€æŸå¤±å¦‚ä½•ï¼Œéƒ½å¿…é¡»è¦èƒ½å¤Ÿè®¡ç®—å¯¼æ•°ã€‚
>
>ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¿…é¡»åˆ†åˆ«**å®ç°æ¯ä¸€å±‚**ã€‚

### What every layer should implement   **æ¯ä¸ªå±‚åº”è¯¥å®ç°ä»€ä¹ˆ**

Every layer that we might create (fully connected, convolutional, maxpooling, dropout, etc.) have at least 2 things in common : **input** and **output** data.

> æˆ‘ä»¬å¯èƒ½æ„å»ºçš„æ¯ä¸€å±‚ï¼ˆå®Œå…¨è¿æ¥ï¼Œå·ç§¯ï¼Œæœ€å¤§åŒ–ï¼Œä¸¢å¤±ç­‰ï¼‰è‡³å°‘æœ‰ä¸¤ä¸ªå…±åŒç‚¹ï¼šè¾“å…¥å’Œè¾“å‡ºæ•°æ®ã€‚



![img](.\Pic\2.png)

#### **Now the important part**        **ç°åœ¨é‡è¦çš„ä¸€éƒ¨åˆ†**

Suppose that we give a layer the **derivative of the error with respect to its output** (âˆ‚E/âˆ‚Y), then it must be able to provide the **derivative of the error with respect to its input** (âˆ‚E/âˆ‚X).

> å‡è®¾ç»™å‡ºä¸€ä¸ªå±‚ç›¸å¯¹äºå…¶è¾“å‡ºï¼ˆâˆ‚E/âˆ‚Yï¼‰è¯¯å·®çš„å¯¼æ•°ï¼Œé‚£ä¹ˆå®ƒå¿…é¡»èƒ½å¤Ÿæä¾›ç›¸å¯¹äºå…¶è¾“å…¥ï¼ˆâˆ‚E/âˆ‚Xï¼‰è¯¯å·®çš„å¯¼æ•°ã€‚



![img](.\Pic\3.png)

Remember that `E` is a **scalar** (a number) and `X` and `Y` are **matrices**.



![img](.\Pic\4.png)

We can easily calculate the elements of âˆ‚E/âˆ‚X using the chain rule :

> æˆ‘ä»¬å¯ä»¥ä½¿ç”¨é“¾è§„åˆ™è½»æ¾è®¡ç®—âˆ‚E/âˆ‚Xçš„å…ƒç´ ï¼š



![img](.\Pic\5.png)

#### Why âˆ‚E/âˆ‚X ?      **ä¸ºä»€ä¹ˆæ˜¯âˆ‚E/âˆ‚Xï¼Ÿ**

For each layer we need the derivative of the error with respect to its **input**because its going to be the derivative of the error with respect to the **previous layerâ€™s output**. This is **very** important, itâ€™s the **key** to understand backpropagation ! After that, weâ€™ll be able to code a Deep Convolutional Neural Network from scratch in no time !

> å¯¹äºæ¯ä¸€å±‚ï¼Œæˆ‘ä»¬éœ€è¦ç›¸å¯¹äºå…¶è¾“å…¥çš„è¯¯å·®å¯¼æ•°ï¼Œå› ä¸ºå®ƒå°†æ˜¯ç›¸å¯¹äºå‰ä¸€å±‚è¾“å‡ºçš„è¯¯å·®å¯¼æ•°ã€‚è¿™éå¸¸é‡è¦ï¼Œè¿™æ˜¯ç†è§£åå‘ä¼ æ’­çš„å…³é”®ï¼åœ¨è¿™ä¹‹åï¼Œæˆ‘ä»¬å°†èƒ½å¤Ÿç«‹å³ä»å¤´å¼€å§‹ç¼–å†™æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼

#### Fancy diagrams        **èŠ±æ ·å›¾è§£**

Essentially, for forward propagation, we give the input data to the first layer, then the output of every layer becomes the input of the next layer until we reach the end of the network.

> åŸºæœ¬ä¸Šï¼Œå¯¹äºå‰å‘ä¼ æ’­ï¼Œæˆ‘ä»¬å°†è¾“å…¥æ•°æ®æä¾›ç»™ç¬¬ä¸€å±‚ï¼Œç„¶åæ¯å±‚çš„è¾“å‡ºæˆä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ï¼Œç›´åˆ°åˆ°è¾¾ç½‘ç»œçš„æœ«ç«¯ã€‚

![](.\Pic\6.png)



For backward propagation, we are simply using the chain rule to get the derivatives we need. This is why every layer must provide the derivative of its output with respect to its input.

> å¯¹äºåå‘ä¼ æ’­ï¼Œæˆ‘ä»¬åªæ˜¯ç®€å•ä½¿ç”¨é“¾è§„åˆ™æ¥è·å¾—éœ€è¦çš„å¯¼æ•°ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ¯ä¸€å±‚å¿…é¡»æä¾›å…¶è¾“å‡ºç›¸å¯¹äºå…¶è¾“å…¥çš„å¯¼æ•°ã€‚



![img](.\Pic\7.png)

This may seem abstract here, but it will get very clear when we will apply this to a specific type of layer. Speaking of *abstract*, now is a good time to write our first python class.

> è¿™å¯èƒ½çœ‹èµ·æ¥å¾ˆæŠ½è±¡ï¼Œä½†æ˜¯å½“æˆ‘ä»¬å°†å…¶åº”ç”¨äºç‰¹å®šç±»å‹çš„å±‚æ—¶ï¼Œå®ƒå°†å˜å¾—éå¸¸æ¸…æ¥šã€‚ç°åœ¨æ˜¯ç¼–å†™ç¬¬ä¸€ä¸ªpythonç±»çš„å¥½æ—¶æœºã€‚

#### Abstract Base Class : Layer          **æŠ½è±¡åŸºç±»ï¼šLayer**

The abstract class *Layer*, which all other layers will inherit from, handles simple properties which are an **input**, an **output**, and both a **forward** and **backward** methods.

> æ‰€æœ‰å…¶å®ƒå±‚å°†ç»§æ‰¿çš„æŠ½è±¡ç±»Layerä¼šå¤„ç†ç®€å•å±æ€§ï¼Œè¿™äº›å±æ€§æ˜¯**è¾“å…¥ï¼Œè¾“å‡º**ä»¥åŠ**å‰å‘**å’Œ**åå‘**æ–¹æ³•ã€‚

```python
from abc import abstractmethod

# Base class
class Layer:
    def __init__(self):
        self.input = None;
        self.output = None;
        self.input_shape = None;
        self.output_shape = None;

    # computes the output Y of a layer for a given input X
    @abstractmethod
    def forward_propagation(self, input):
        raise NotImplementedError

    # computes dE/dX for a given dE/dY (and update parameters if any)
    @abstractmethod
    def backward_propagation(self, output_error, learning_rate):
        raise NotImplementedError
```

As you can see there is an extra parameter in `backward_propagation` that I didnâ€™t mention, it is the `learning_rate`. This parameter should be something like an update policy, or an optimizer as they call it in Keras, but for the sake of simplicity weâ€™re simply going to pass a learning rate and update our parameters using gradient descent.

> æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œåœ¨ back_propagation å‡½æ•°ä¸­ï¼Œæœ‰ä¸€ä¸ªæˆ‘æ²¡æœ‰æåˆ°çš„å‚æ•°ï¼Œå®ƒæ˜¯ learning_rate ã€‚ æ­¤å‚æ•°åº”è¯¥ç±»ä¼¼äºæ›´æ–°ç­–ç•¥æˆ–è€…åœ¨Kerasä¸­è°ƒç”¨å®ƒçš„ä¼˜åŒ–å™¨ï¼Œä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åªæ˜¯é€šè¿‡å­¦ä¹ ç‡å¹¶ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°æˆ‘ä»¬çš„å‚æ•°ã€‚

### Fully Connected Layer     **å…¨è¿æ¥å±‚**

Now lets define and implement the first type of layer : fully connected layer or FC layer. FC layers are the most basic layers as every input neurons are connected to every output neurons.

> ç°åœ¨å…ˆå®šä¹‰å¹¶å®ç°ç¬¬ä¸€ç§ç±»å‹çš„ç½‘ç»œå±‚ï¼šå…¨è¿æ¥å±‚æˆ–FCå±‚ã€‚FCå±‚æ˜¯æœ€åŸºæœ¬çš„ç½‘ç»œå±‚ï¼Œå› ä¸ºæ¯ä¸ªè¾“å…¥ç¥ç»å…ƒéƒ½è¿æ¥åˆ°æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒã€‚



![img](.\Pic\8.png)

#### Forward Propagation    **å‰å‘ä¼ æ’­**

The value of each output neuron can be calculated as the following :

> æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒçš„å€¼ç”±ä¸‹å¼è®¡ç®—ï¼š

![](.\Pic\9.png)



With matrices, we can compute this formula for every output neuron in one shot using a **dot product** :

> ä½¿ç”¨çŸ©é˜µï¼Œå¯ä»¥ä½¿ç”¨ç‚¹ç§¯æ¥è®¡ç®—æ¯ä¸€ä¸ªè¾“å‡ºç¥ç»å…ƒçš„å€¼ï¼š



![img](.\Pic\10.png)





Weâ€™re done with the forward pass. Now letâ€™s do the backward pass of the FC layer.

> å½“å®Œæˆå‰å‘ä¼ æ’­ä¹‹åï¼Œç°åœ¨å¼€å§‹åšåå‘ä¼ æ’­ã€‚

------

*Note that Iâ€™m not using any activation function yet, thatâ€™s because we will implement it in a separate layer !*

------

#### Backward Propagation     **åå‘ä¼ æ’­**

As we said, suppose we have a matrix containing the derivative of the error with respect to **that layerâ€™s output** (âˆ‚E/âˆ‚Y). We need :

> æ­£å¦‚æˆ‘ä»¬æ‰€è¯´ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªçŸ©é˜µï¼Œå…¶ä¸­åŒ…å«ä¸è¯¥å±‚è¾“å‡ºç›¸å…³çš„è¯¯å·®å¯¼æ•°ï¼ˆâˆ‚E/âˆ‚Yï¼‰ã€‚ æˆ‘ä»¬éœ€è¦ ï¼š

1. The derivative of the error with respect to the parameters (âˆ‚E/âˆ‚W, âˆ‚E/âˆ‚B)
2. The derivative of the error with respect to the input (âˆ‚E/âˆ‚X)



> 1.å…³äºå‚æ•°çš„è¯¯å·®å¯¼æ•°ï¼ˆâˆ‚E/âˆ‚Wï¼Œâˆ‚E/âˆ‚Bï¼‰
>
> 2.å…³äºè¾“å…¥çš„è¯¯å·®å¯¼æ•°ï¼ˆâˆ‚E/âˆ‚Xï¼‰

Lets calculate âˆ‚E/âˆ‚W. This matrix should be the same size as W itself : `ixj`where `i` is the number of input neurons and `j` the number of output neurons. We need **one gradient for every weight** :

>é¦–å…ˆè®¡ç®—âˆ‚E/âˆ‚Wï¼Œè¯¥çŸ©é˜µåº”ä¸Wæœ¬èº«çš„å¤§å°ç›¸åŒï¼šå¯¹äºixjï¼Œå…¶ä¸­iæ˜¯è¾“å…¥ç¥ç»å…ƒçš„æ•°é‡ï¼Œjæ˜¯è¾“å‡ºç¥ç»å…ƒçš„æ•°é‡ã€‚æ¯ä¸ª**æƒé‡éƒ½éœ€è¦ä¸€ä¸ªæ¢¯åº¦**ï¼š



![img](.\Pic\11.png)

Using the chain rule stated earlier, we can write :



![img](.\Pic\12.png)

Therefore,



![img](.\Pic\13.png)

Thatâ€™s it we have the first formula to update the weights ! Now lets calculate âˆ‚E/âˆ‚B.

![](.\Pic\14.png)



Again âˆ‚E/âˆ‚B needs to be of the same size as B itself, one gradient per bias. We can use the chain rule again :



![img](.\Pic\15.png)

And conclude that,



![img](.\Pic\16.png)

Now that we have **âˆ‚E/âˆ‚W** and **âˆ‚E/âˆ‚B**, we are left with **âˆ‚E/âˆ‚X** which is **very important** as it will â€œactâ€ as âˆ‚E/âˆ‚Y for the layer before that one.

![](.\Pic\17.png)



Again, using the chain rule,



![img](.\Pic\18.png)

Finally, we can write the whole matrix :



![img](.\Pic\19.png)

Thatâ€™s it ! We have the three formulas we needed for the FC layer !



![img](C:\Users\liumian\Documents\codes\codes\00GitHub\Medium-Python-Neural-Network\Pic\20.png)

#### Coding the Fully Connected Layer   **ç¼–ç å…¨è¿æ¥å±‚**

We can now write some python code to bring this math to life !

```python
from layer import Layer
import numpy as np

# inherit from base class Layer
class FCLayer(Layer):
    # input_shape = (1,i)   i the number of input neurons
    # output_shape = (1,j)  j the number of output neurons
    def __init__(self, input_shape, output_shape):
        self.input_shape = input_shape;
        self.output_shape = output_shape;
        self.weights = np.random.rand(input_shape[1], output_shape[1]) - 0.5;
        self.bias = np.random.rand(1, output_shape[1]) - 0.5;

    # returns output for a given input
    def forward_propagation(self, input):
        self.input = input;
        self.output = np.dot(self.input, self.weights) + self.bias;
        return self.output;

    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.
    def backward_propagation(self, output_error, learning_rate):
        input_error = np.dot(output_error, self.weights.T);
        dWeights = np.dot(self.input.T, output_error);
        # dBias = output_error
        
        # update parameters
        self.weights -= learning_rate * dWeights;
        self.bias -= learning_rate * output_error;
        return input_error;
```



### Activation Layer      **æ¿€æ´»å±‚**

All the calculation we did until now were completely linear. Its hopeless to learn anything with that kind of model. We need to add **non-linearity** to the model by applying non linear functions to the output of some layers.

Now we need to redo the whole process for this new type of layer !

No worries, itâ€™s going to be way faster as there are no *learnable* parameters. We just need to calculate **âˆ‚E/âˆ‚X**.

>åˆ°ç›®å‰ä¸ºæ­¢æ‰€åšçš„è®¡ç®—éƒ½å®Œå…¨æ˜¯çº¿æ€§çš„ã€‚ç”¨è¿™ç§æ¨¡å‹å­¦ä¹ æ˜¯æ²¡æœ‰å¸Œæœ›çš„ï¼Œéœ€è¦é€šè¿‡å°†éçº¿æ€§å‡½æ•°åº”ç”¨äºæŸäº›å±‚çš„è¾“å‡ºæ¥ä¸ºæ¨¡å‹æ·»åŠ éçº¿æ€§ã€‚
>
>ç°åœ¨æˆ‘ä»¬éœ€è¦ä¸ºè¿™ç§æ–°ç±»å‹çš„å±‚ï¼ˆæ¿€æ´»å±‚ï¼‰é‡åšæ•´ä¸ªè¿‡ç¨‹ï¼
>
>ä¸ç”¨æ‹…å¿ƒï¼Œå› ä¸ºæ­¤æ—¶æ²¡æœ‰å¯å­¦ä¹ çš„å‚æ•°ï¼Œè¿‡ç¨‹ä¼šå¿«ç‚¹ï¼Œåªéœ€è¦è®¡ç®—âˆ‚E/âˆ‚Xã€‚

We will call `f` and `f'` the activation function and its derivative respectively.



![img](.\Pic\21.png)

#### Forward Propagation  **å‰å‘ä¼ æ’­**

As you will see, it is quite straightforward. For a given input `X` , the output is simply the activation function applied to every element of `X` . Which means **input** and **output** have the **same dimensions**.

> æ­£å¦‚å°†çœ‹åˆ°çš„ï¼Œå®ƒéå¸¸ç®€å•ã€‚å¯¹äºç»™å®šçš„è¾“å…¥Xï¼Œè¾“å‡ºæ˜¯å…³äºæ¯ä¸ªXå…ƒç´ çš„æ¿€æ´»å‡½æ•°ï¼Œè¿™æ„å‘³ç€è¾“å…¥å’Œè¾“å‡ºå…·æœ‰ç›¸åŒçš„å¤§å°ã€‚



![img](.\Pic\22.png)

#### Backward Propagation    **åå‘ä¼ æ’­**

Given **âˆ‚E/âˆ‚Y**, we want to calculate **âˆ‚E/âˆ‚X**.



![img](.\Pic\23.png)

Be careful, here we are using an **element-wise** multiplication between the two matrices (whereas in the formulas above, it was a dot product).

#### Coding the Activation Layer        **ç¼–ç å®ç°æ¿€æ´»å±‚**

The code for the activation layer is as straightforward.

```python
from layer import Layer

# inherit from base class Layer
class ActivationLayer(Layer):
    # input_shape = (1,i)   i the number of input neurons
    def __init__(self, input_shape, activation, activation_prime):
        self.input_shape = input_shape;
        self.output_shape = input_shape;
        self.activation = activation;
        self.activation_prime = activation_prime;

    # returns the activated input
    def forward_propagation(self, input):
        self.input = input;
        self.output = self.activation(self.input);
        return self.output;

    # Returns input_error=dE/dX for a given output_error=dE/dY.
    # learning_rate is not used because there is no "learnable" parameters.
    def backward_propagation(self, output_error, learning_rate):
        return self.activation_prime(self.input) * output_error;
```



You can also write some activation functions and their derivatives in a separate file. These will be used later to create an `ActivationLayer`.

```python
import numpy as np

# activation function and its derivative
def tanh(x):
    return np.tanh(x);

def tanh_prime(x):
    return 1-np.tanh(x)**2;
```


### Loss Function      **æŸå¤±å‡½æ•°**

Until now, for a given layer, we supposed that **âˆ‚E/âˆ‚Y** was given (by the next layer). But what happens to the last layer ? How does it get **âˆ‚E/âˆ‚Y** ? We simply give it manually, and it depends on how we define the error.

The error of the network, which measures how good or bad the network did for a given input data, is defined by **you**. There are many ways to define the error, and one of the most known is called **MSEâ€Šâ€”â€ŠMean Squared Error**.

>åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå¯¹äºç»™å®šçš„å±‚ï¼Œæˆ‘ä»¬å‡è®¾ç»™å‡ºäº†âˆ‚E/âˆ‚Yï¼ˆç”±ä¸‹ä¸€å±‚ç»™å‡ºï¼‰ã€‚ä½†æ˜¯æœ€åä¸€å±‚æ€ä¹ˆå¾—åˆ°âˆ‚E/âˆ‚Yï¼Ÿæˆ‘ä»¬é€šè¿‡ç®€å•åœ°æ‰‹åŠ¨ç»™å‡ºæœ€åä¸€å±‚çš„âˆ‚E/âˆ‚Yï¼Œå®ƒå–å†³äºæˆ‘ä»¬å¦‚ä½•å®šä¹‰è¯¯å·®ã€‚
>
>ç½‘ç»œçš„è¯¯å·®ç”±è‡ªå·±å®šä¹‰ï¼Œè¯¥è¯¯å·®è¡¡é‡ç½‘ç»œå¯¹ç»™å®šè¾“å…¥æ•°æ®çš„å¥½åç¨‹åº¦ã€‚æœ‰è®¸å¤šæ–¹æ³•å¯ä»¥å®šä¹‰è¯¯å·®ï¼Œå…¶ä¸­ä¸€ç§æœ€å¸¸è§çš„å«åšMSE - Mean Squared Errorï¼š



![img](.\Pic\24.png)



Where `y*` and `y` denotes **desired output** and **actual output** respectively. You can think of the loss as a last layer which takes all the output neurons and squashes them into one single neuron. What we need now, as for every other layer, is to define **âˆ‚E/âˆ‚Y**. Except now, we finally reached `E` !

> å…¶ä¸­y *å’Œyåˆ†åˆ«è¡¨ç¤ºæœŸæœ›çš„è¾“å‡ºå’Œå®é™…è¾“å‡ºã€‚ä½ å¯ä»¥å°†æŸå¤±è§†ä¸ºæœ€åä¸€å±‚ï¼Œå®ƒå°†æ‰€æœ‰è¾“å‡ºç¥ç»å…ƒå¸æ”¶å¹¶å°†å®ƒä»¬å‹æˆä¸€ä¸ªç¥ç»å…ƒã€‚ä¸å…¶ä»–æ¯ä¸€å±‚ä¸€æ ·ï¼Œéœ€è¦å®šä¹‰âˆ‚E/âˆ‚Yã€‚é™¤äº†ç°åœ¨ï¼Œæˆ‘ä»¬ç»ˆäºå¾—åˆ°Eï¼



![img](.\Pic\25.png)

These are simply two python functions that you can put in a separate file. They will be used when creating the network.

```python
import numpy as np

# loss function and its derivative
def mse(y_true, y_pred):
    return np.mean(np.power(y_true-y_pred, 2));

def mse_prime(y_true, y_pred):
    return 2*(y_pred-y_true)/y_true.size;
```





### Network Class    **ç½‘ç»œç±»**

Almost done ! We are going to make a `Network` class to create neural networks very easily akin the first picture !

I commented almost every part of the code, it shouldnâ€™t be too complicated to understand if you grasped the previous steps. Nevertheless, leave a comment if you have any question, I will gladly answer !

>åˆ°ç°åœ¨å‡ ä¹å®Œæˆäº†ï¼æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªNetworkç±»æ¥åˆ›å»ºç¥ç»ç½‘ç»œï¼Œéå¸¸å®¹æ˜“ï¼Œç±»ä¼¼äºç¬¬ä¸€å¼ å›¾ç‰‡ï¼
>
>æˆ‘æ³¨é‡Šäº†ä»£ç çš„æ¯ä¸€éƒ¨åˆ†ï¼Œå¦‚æœä½ æŒæ¡äº†å‰é¢çš„æ­¥éª¤ï¼Œé‚£ä¹ˆç†è§£å®ƒåº”è¯¥ä¸ä¼šå¤ªå¤æ‚ã€‚

```python
from layer import Layer

class Network:
    def __init__(self):
        self.layers = [];
        self.loss = None;
        self.loss_prime = None;

    # add layer to network
    def add(self, layer):
        self.layers.append(layer);

    # set loss to use
    def use(self, loss, loss_prime):
        self.loss = loss;
        self.loss_prime = loss_prime;

    # predict output for given input
    def predict(self, input):
        # sample dimension first
        samples = len(input);
        result = [];

        # run network over all samples
        for i in range(samples):
            # forward propagation
            output = input[i];
            for layer in self.layers:
                # output of layer l is input of layer l+1
                output = layer.forward_propagation(output);
            result.append(output);

        return result;

    # train the network
    def fit(self, x_train, y_train, epochs, learning_rate):
        # sample dimension first
        samples = len(x_train);

        # training loop
        for i in range(epochs):
            err = 0;
            for j in range(samples):
                # forward propagation
                output = x_train[j];
                for layer in self.layers:
                    output = layer.forward_propagation(output);

                # compute loss (for display purpose only)
                err += self.loss(y_train[j], output);

                # backward propagation
                error = self.loss_prime(y_train[j], output);
                # loop from end of network to beginning
                for layer in reversed(self.layers):
                    # backpropagate dE
                    error = layer.backward_propagation(error, learning_rate);

            # calculate average error on all samples
            err /= samples;
            print('epoch %d/%d   error=%f' % (i+1,epochs,err));
```



### Building a Neural Network     **æ„å»ºä¸€ä¸ªç¥ç»ç½‘ç»œ**

Finally ! We can use our class to create a neural network with as many layers as we want ! For the sake of simplicity, Iâ€™m just going to show you how to makeâ€¦ a **XOR**.

> æœ€åï¼æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„ç±»æ¥åˆ›å»ºä¸€ä¸ªåŒ…å«ä»»æ„æ•°é‡å±‚çš„ç¥ç»ç½‘ç»œï¼ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘å°†å‘ä½ å±•ç¤ºå¦‚ä½•æ„å»º......ä¸€ä¸ªXORã€‚

```python
from network import Network
from fc_layer import FCLayer
from activation_layer import ActivationLayer
from losses import *
from activations import *
import numpy as np

# training data
x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]]);
y_train = np.array([[[0]], [[1]], [[1]], [[0]]]);

# network
net = Network();
net.add(FCLayer((1,2), (1,3)));
net.add(ActivationLayer((1,3), tanh, tanh_prime));
net.add(FCLayer((1,3), (1,1)));
net.add(ActivationLayer((1,1), tanh, tanh_prime));

# train
net.use(mse, mse_prime);
net.fit(x_train, y_train, epochs=1000, learning_rate=0.1);

# test
out = net.predict(x_train);
print(out);
```



Again, I donâ€™t think I need to emphasize many things. Just be careful with the training data, you should always have the **sample** dimension **first**. For example, with the xor problem, the shape should be (4,1,2).

> åŒæ ·ï¼Œæˆ‘è®¤ä¸ºä¸éœ€è¦å¼ºè°ƒå¾ˆå¤šäº‹æƒ…ï¼Œåªéœ€è¦ä»”ç»†è®­ç»ƒæ•°æ®ï¼Œåº”è¯¥èƒ½å¤Ÿå…ˆè·å¾—æ ·æœ¬ç»´åº¦ã€‚ä¾‹å¦‚ï¼Œå¯¹äºxoré—®é¢˜ï¼Œæ ·å¼åº”ä¸ºï¼ˆ4,1,2ï¼‰ã€‚

#### Result

```
$ python xor.py 
epoch 1/1000 error=0.322980
epoch 2/1000 error=0.311174
epoch 3/1000 error=0.307195
...
epoch 998/1000 error=0.000243
epoch 999/1000 error=0.000242
epoch 1000/1000 error=0.000242
[array([[ 0.00077435]]), array([[ 0.97760742]]), array([[ 0.97847793]]), array([[-0.00131305]])]
```

### Convolutional Layer     **å·ç§¯å±‚**

This post is starting to be pretty long so I wonâ€™t describe all the steps to implement a convolutional layer. However, hereâ€™s an implementation that I made :

> è¿™ç¯‡æ–‡ç« å¼€å§‹å¾ˆé•¿ï¼Œæ‰€ä»¥æˆ‘ä¸ä¼šæè¿°å®ç°å·ç§¯å±‚çš„æ‰€æœ‰æ­¥éª¤ã€‚ä½†æ˜¯ï¼Œè¿™æ˜¯æˆ‘åšçš„ä¸€ä¸ªå®ç°ï¼š

```python
from layer import Layer
from scipy import signal
import numpy as np

# inherit from base class Layer
# This convolutional layer is always with stride 1
class ConvLayer(Layer):
    # input_shape = (i,j,d)
    # kernel_shape = (m,n)
    # layer_depth = output depth
    def __init__(self, input_shape, kernel_shape, layer_depth):
        self.input_shape = input_shape;
        self.input_depth = input_shape[2];
        self.kernel_shape = kernel_shape;
        self.layer_depth = layer_depth;
        self.output_shape = (input_shape[0]-kernel_shape[0]+1, input_shape[1]-kernel_shape[1]+1, layer_depth);
        self.weights = np.random.rand(kernel_shape[0], kernel_shape[1], self.input_depth, layer_depth) - 0.5;
        self.bias = np.random.rand(layer_depth) - 0.5;

    # returns output for a given input
    def forward_propagation(self, input):
        self.input = input;
        self.output = np.zeros(self.output_shape);

        for k in range(self.layer_depth):
            for d in range(self.input_depth):
                self.output[:,:,k] += signal.correlate2d(self.input[:,:,d], self.weights[:,:,d,k], 'valid') + self.bias[k];

        return self.output;

    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.
    def backward_propagation(self, output_error, learning_rate):
        in_error = np.zeros(self.input_shape);
        dWeights = np.zeros((self.kernel_shape[0], self.kernel_shape[1], self.input_depth, self.layer_depth));
        dBias = np.zeros(self.layer_depth);

        for k in range(self.layer_depth):
            for d in range(self.input_depth):
                in_error[:,:,d] += signal.convolve2d(output_error[:,:,k], self.weights[:,:,d,k], 'full');
                dWeights[:,:,d,k] = signal.correlate2d(self.input[:,:,d], output_error[:,:,k], 'valid');
            dBias[k] = self.layer_depth * np.sum(output_error[:,:,k]);

        self.weights -= learning_rate*dWeights;
        self.bias -= learning_rate*dBias;
        return in_error;
```





The math behind it is actually not very complicated ! Here is an excellent post where youâ€™ll find explanations and calculations for **âˆ‚E/âˆ‚W, âˆ‚E/âˆ‚B** and **âˆ‚E/âˆ‚X**.

>å®ƒèƒŒåçš„æ•°å­¦å®é™…ä¸Šå¹¶ä¸å¤æ‚ï¼è¿™æ˜¯ä¸€ç¯‡å¾ˆå¥½çš„æ–‡ç« ï¼Œä½ å¯ä»¥æ‰¾åˆ°âˆ‚E/âˆ‚Wï¼Œâˆ‚E/âˆ‚Bå’Œâˆ‚E/âˆ‚Xçš„è§£é‡Šå’Œè®¡ç®—ã€‚
>
>å¦‚æœä½ æƒ³éªŒè¯ä½ çš„ç†è§£æ˜¯å¦æ­£ç¡®ï¼Œè¯·å°è¯•è‡ªå·±å®ç°ä¸€äº›ç½‘ç»œå±‚ï¼Œå¦‚MaxPoolingï¼ŒFlattenæˆ–Dropout

[**Forward And Backpropagation in Convolutional Neural Network.**
*The below post demonstrates the use of convolution operation for carrying out the back propagation in a CNN.*medium.com](https://medium.com/@2017csm1006/forward-and-backpropagation-in-convolutional-neural-network-4dfa96d7b37e)

If youâ€™d like to check your understanding, try to implement some layers for yourself like MaxPooling, Flatten, or Dropout.

### GitHub Repository

You can find the whole working code used for this post on the following GitHub repository.

[**OmarAflak/Medium-Python-Neural-Network**
*Contribute to OmarAflak/Medium-Python-Neural-Network development by creating an account on GitHub.*github.com](https://github.com/OmarAflak/Medium-Python-Neural-Network)

------

### **If you liked this postâ€Šâ€”â€ŠIâ€™d really appreciate if you hit the clap button** ğŸ‘ **it would help me a lot. Peace! ğŸ˜**